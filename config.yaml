# Stage A Configuration
# Based on config.example.yaml

stage_a:
  # Root directory for raw Parquet files
  parquet_raw_root: /home/mingyuan/data/taq/parquet_raw
  
  # WRDS username (optional, will prompt if not provided)
  wrds_username: mingyuancu
  
  # Extraction settings
  chunk_size: 50  # Number of symbols to process per chunk from WRDS
  streaming_chunk_rows: 1000000  # Rows per chunk
  
  # Parquet settings
  compression: snappy  # Options: snappy, gzip, zstd, lz4
  partition_by_symbol: true  # Partition by symbol subdirectory
  
  # Timezone for timestamp conversion
  timezone: America/New_York

# Stage A Alpaca Configuration
stage_a_alpaca:
  # Root directory for raw Parquet files (separate from TAQ data)
  parquet_raw_root: /home/mingyuan/data/alpaca/parquet_raw
  
  # TAQ data root for symbol discovery (optional)
  # If --symbols is not provided, symbols will be discovered from this directory
  taq_parquet_root: /home/mingyuan/data/taq/parquet_raw
  
  # Alpaca API credentials
  # For security, use one of these methods (in order of priority):
  # 1. Environment variables: ALPACA_API_KEY, ALPACA_SECRET_KEY (recommended)
  # 2. config.secrets.yaml file (create from config.secrets.example.yaml)
  # 3. Leave as null here - do NOT commit actual keys to git!
  alpaca_api_key: null  # Set in config.secrets.yaml or environment variables
  alpaca_secret_key: null  # Set in config.secrets.yaml or environment variables
  
  # Alpaca API base URL
  # Use "https://data.alpaca.markets" for historical data (free tier)
  # Use "https://paper-api.alpaca.markets" for paper trading (real-time)
  # Use "https://api.alpaca.markets" for live trading (requires paid account)
  alpaca_base_url: https://data.alpaca.markets
  
  # Data feed (use "sip" for consolidated SIP data)
  feed: sip
  
  # Extraction settings
  chunk_size: 50  # Number of symbols to process per chunk
  streaming_chunk_rows: 1000000  # Rows per chunk
  
  # Parquet settings
  compression: snappy  # Options: snappy, gzip, zstd, lz4
  partition_by_symbol: true  # Partition by symbol subdirectory
  
  # Timezone for timestamp conversion
  timezone: America/New_York
  
  # API settings
  page_limit: 10000  # Max records per API call

# Stage A Alpaca IEX Configuration
stage_a_alpaca_iex:
  # Root directory for raw Parquet files (separate from TAQ, Alpaca SIP, and CSV data)
  parquet_raw_root: /home/mingyuan/data/alpaca_iex/parquet_raw
  
  # Alpaca API credentials (shared with stage_a_alpaca, set in config.secrets.yaml)
  # These will use the same credentials as stage_a_alpaca if not specified
  alpaca_api_key: null  # Set in config.secrets.yaml or environment variables
  alpaca_secret_key: null  # Set in config.secrets.yaml or environment variables
  
  # Alpaca API base URL
  # Use "https://data.alpaca.markets" for historical data (free tier)
  alpaca_base_url: https://data.alpaca.markets
  
  # Data feed (IEX feed - available on free tier)
  feed: iex
  
  # Extraction settings
  chunk_size: 50
  streaming_chunk_rows: 1000000
  
  # Parquet settings
  compression: snappy
  partition_by_symbol: true
  
  # Timezone for timestamp conversion
  timezone: America/New_York
  
  # API settings
  page_limit: 10000

# Stage A CSV Configuration
stage_a_csv:
  # Root directory for raw Parquet files (separate from TAQ and Alpaca data)
  parquet_raw_root: /home/mingyuan/data/csv/parquet_raw
  
  # Root directory containing CSV files (can be anywhere - point to where your CSV files are located)
  csv_root: /path/to/your/csv/files
  
  # Extraction settings
  chunk_size: 1000000  # Rows per chunk when reading CSV files
  
  # Parquet settings
  compression: snappy  # Options: snappy, gzip, zstd, lz4
  partition_by_symbol: true  # Partition by symbol subdirectory
  
  # Timezone for timestamp conversion
  timezone: America/New_York
  
  # CSV file naming pattern
  # Files should be named: {prefix}_{date}.csv where date is YYYYMMDD
  csv_prefix_trades: taq_trade
  csv_prefix_quotes: taq_quote
  csv_prefix_nbbo: taq_nbbo

